---
title: "Quality of Activity - Appendices"
author: "Michael Szczepaniak"
date: "November 2015"
output: html_document
---

## Appendix A - Excluded Variables

The following code was used to determine which columns had missing data:  

```{r eval=FALSE}

trainFile <- "https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-training.csv"
testFile <- "https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-testing.csv"
rawActitivity <- read.csv(trainFile)

## Creates a data frame with three columns: index, ColumnName and
## FractionMissing.
## index is the column index in df corresponding to ColumnName
## ColumnName is as the name implies: the name the column in df
## FractionMissing is the fraction of values that are missing or NA.
## The closer this value is to 1, the less data the column contains
getFractionMissing <- function(df = rawActitivity) {
    colCount <- ncol(df)
    returnDf <- data.frame(index=1:ncol(df),
                           columnName=rep("undefined", colCount),
                           FractionMissing=rep(-1, colCount),
                           stringsAsFactors=FALSE)
    for(i in 1:colCount) {
        colVector <- df[,i]
        missingCount <- length(which(colVector == "") * 1)
        missingCount <- missingCount + sum(is.na(colVector) * 1)
        returnDf$columnName[i] <- as.character(names(df)[i])
        returnDf$FractionMissing[i] <- missingCount / length(colVector)
    }
    
    return(returnDf)
}

varsDf <- getFractionMissing()
varsDf

```

Base on the output above, the following variables were excluded for two reasons. First, `r round(19216/19622, 2) * 100` % of their values were missing. Second, these same variables did were not populated with values in the testing data set (running *getFractionMissing(pml_testing)*).

kurtosis_roll_belt  
kurtosis_picth_belt  
kurtosis_yaw_belt  
skewness_roll_belt  
skewness_roll_belt.1  
skewness_yaw_belt  
max_roll_belt  
max_picth_belt  
max_yaw_belt  
min_roll_belt  
min_pitch_belt  
min_yaw_belt  
amplitude_roll_belt  
amplitude_pitch_belt  
amplitude_yaw_belt  
var_total_accel_belt  
avg_roll_belt  
stddev_roll_belt  
var_roll_belt  
avg_pitch_belt  
stddev_pitch_belt  
var_pitch_belt  
avg_yaw_belt  
stddev_yaw_belt  
var_yaw_belt  
var_accel_arm  
avg_roll_arm  
stddev_roll_arm  
var_roll_arm  
avg_pitch_arm  
stddev_pitch_arm  
var_pitch_arm  
avg_yaw_arm  
stddev_yaw_arm  
var_yaw_arm  
kurtosis_roll_arm  
kurtosis_picth_arm  
kurtosis_yaw_arm  
skewness_roll_arm  
skewness_pitch_arm  
skewness_yaw_arm  
max_roll_arm  
max_picth_arm  
max_yaw_arm  
min_roll_arm  
min_pitch_arm  
min_yaw_arm  
amplitude_roll_arm  
amplitude_pitch_arm  
amplitude_yaw_arm  
kurtosis_roll_dumbbell  
kurtosis_picth_dumbbell  
kurtosis_yaw_dumbbell  
skewness_roll_dumbbell  
skewness_pitch_dumbbell  
skewness_yaw_dumbbell  
max_roll_dumbbell  
max_picth_dumbbell  
max_yaw_dumbbell  
min_roll_dumbbell  
min_pitch_dumbbell  
min_yaw_dumbbell  
amplitude_roll_dumbbell  
amplitude_pitch_dumbbell  
amplitude_yaw_dumbbell  
var_accel_dumbbell  
avg_roll_dumbbell  
stddev_roll_dumbbell  
var_roll_dumbbell  
avg_pitch_dumbbell  
stddev_pitch_dumbbell  
var_pitch_dumbbell  
avg_yaw_dumbbell  
stddev_yaw_dumbbell  
var_yaw_dumbbell  
kurtosis_roll_forearm  
kurtosis_picth_forearm  
kurtosis_yaw_forearm  
skewness_roll_forearm  
skewness_pitch_forearm  
skewness_yaw_forearm  
max_roll_forearm  
max_picth_forearm  
max_yaw_forearm  
min_roll_forearm  
min_pitch_forearm  
min_yaw_forearm  
amplitude_roll_forearm  
amplitude_pitch_forearm  
amplitude_yaw_forearm  
var_accel_forearm  
avg_roll_forearm  
stddev_roll_forearm  
var_roll_forearm  
avg_pitch_forearm  
stddev_pitch_forearm  
var_pitch_forearm  
avg_yaw_forearm  
stddev_yaw_forearm  
var_yaw_forearm  

The following code was used to create the data set with only the selected predictors.  

```{r eval=FALSE}
library(caret); library(dplyr)
varsDf <- getFractionMissing()
# get column/var names with > 80% of values not empty or NA
varsWithData <- filter(varsDf, FractionMissing < 0.20)$columnName
# varsWithLittleData <- filter(varsDf, FractionMissing > 0.90)$columnName
# http://stackoverflow.com/questions/10086494#10086494
trimmedActivity <- subset(rawActitivity, select=varsWithData)
# This eliminated 100 out of the 160 vars. Turns out that these same
# 100 var's aren't populated at all in test set which makes it a
# no-brainer to pitch them.
offset = 6; classDesignatorCol <- ncol(trimmedActivity) # last col is class
# convert new_window from factor to int
trimmedActivity$new_window <- as.integer(trimmedActivity$new_window)
correlationMatrix <- cor(trimmedActivity[, (offset+1):classDesignatorCol-1])
#print(correlationMatrix)
# find attributes that are highly corrected (ideally > 0.75)
highlyCorrelated <- sort(findCorrelation(correlationMatrix, cutoff=0.75))
highlyCorrelated <- highlyCorrelated + offset # get cols relative to trimmed df
trimmedActivity2 <- trimmedActivity[,-highlyCorrelated] # remove highly cor vars
trimmedActivity2 <- trimmedActivity2[,-1] # 1st col is just an index
trimmedActivity3 <- trimmedActivity2[,-4:-2] # remove time stamp info
# remove user_name and new_window after looking at importance plot (see below)
trimmedActivity3 <- trimmedActivity3[,-1] # remove user_name
trimmedActivity3 <- trimmedActivity3[,-1] # remove new_window
```



## Appendix B - Calculation of Error and Accuracy

The following code was used to build models, calculate the errors and calculate accuracy on the validation test set:  

```{r eval=FALSE}
correct.validation.responses <- c("B", "A", "B", "A", "A", "E", "D", "B", "A", "A",
                                  "B", "C", "B", "A", "E", "E", "A", "B", "B", "B")
```

```{r ldaTrainTest, cache=TRUE, eval=FALSE}
# MODEL 1) Start with fast and simple LDA model using caret training defaults
set.seed(1447)
inTrain <- createDataPartition(trimmedActivity3$classe, p=0.60, list=FALSE)
training <- trimmedActivity3[inTrain,]
testing <- trimmedActivity3[-inTrain,]
startTime <- Sys.time()
#cat(format(startTime, "%T"), "building LDA model started...\n")
set.seed(374659)
mod01.lda <- train(classe ~ ., method='lda', data=training) # about 6 sec's
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building LDA model FINISHED!\n")
time.lda.minutes <- endTime - startTime
# http://stackoverflow.com/questions/5396429/getting-consist-units-from-diff-command-in-r
units(time.lda.minutes) <- "mins"
```

```{r ldaErrorsAccuracy, cache=TRUE, eval=FALSE}
# calc in-sample and out-of-sample error
pred.lda.insample <- predict(mod01.lda, newdata = training)
pred.lda.oosample <- predict(mod01.lda, newdata = testing)
acc.lda.insample <- sum(pred.lda.insample == training$classe) / length(training$classe)
acc.lda.oosample <- sum(pred.lda.oosample == testing$classe) / length(testing$classe)
# predict test cases
pred.lda.test.cases <- predict(mod01.lda, newdata = test.cases)
validation.acc.lda <- sum(pred.lda.test.cases == correct.validation.responses) /
                      length(correct.validation.responses)
# > pred01.lda.test.cases
# [1] B A A A C C D D A A D A B A E B A B A B
```

```{r rpartTrainTest, cache=TRUE, eval=FALSE}
suppressMessages(suppressWarnings(library(rpart)))
# MODEL 2) Try a classification tree using caret training defaults
startTime <- Sys.time()
#cat(format(Sys.time(), "%T"), "building CART model started...\n")
set.seed(1449)
mod02.cart <- train(classe ~ ., method='rpart', data=training) # about 9 sec's
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building CART model FINISHED!\n")
time.rpart.minutes <- endTime - startTime
units(time.rpart.minutes) <- "mins"
```

```{r rpartErrorsAccuracy, cache=TRUE, eval=FALSE}
# calc in-sample and out-of-sample error
pred.cart.insample <- predict(mod02.cart, newdata = training)
pred.cart.oosample <- predict(mod02.cart, newdata = testing)
acc.cart.insample <- sum(pred.cart.insample == training$classe) / length(training$classe)
acc.cart.oosample <- sum(pred.cart.oosample == testing$classe) / length(testing$classe)

# predict test cases
pred.cart.test.cases <- predict(mod02.cart, newdata = test.cases)
validation.acc.cart <- sum(pred.cart.test.cases == correct.validation.responses) /
                       length(correct.validation.responses)
# > pred.cart.test.cases
#  [1] E C A A A C C C A A B C B A C B E B E B
```

```{r rfTrainTest, cache=TRUE, eval=FALSE}
suppressMessages(suppressWarnings(library(randomForest)))
# MODEL 3) Try a random forrest classifier
startTime <- Sys.time()
# cat(format(startTime, "%T"), "building RF model started...\n")
set.seed(1451)
mod03.rf <- train(classe ~ ., method='rf', prox=TRUE, data=training)
# above line took 3 hrs. on my Xeon 16Gb workstation to complete
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building RF model FINISHED!\n")
time.rf.minutes <- endTime - startTime
units(time.rf.minutes) <- "mins"
```

```{r rfErrorsAccuracy, cache=TRUE, eval=FALSE}
# calc in-sample and out-of-sample error
pred.rf.insample <- predict(mod03.rf, newdata = training)
pred.rf.oosample <- predict(mod03.rf, newdata = testing)
acc.rf.insample <- sum(pred.rf.insample == training$classe) / length(training$classe)
acc.rf.oosample <- sum(pred.rf.oosample == testing$classe) / length(testing$classe)

# read the test cases and predict them
pred.rf.test.cases <- predict(mod03.rf, newdata = test.cases)
validation.acc.rf <- sum(pred.rf.test.cases == correct.validation.responses) /
                     length(correct.validation.responses)
# > pred.rf.test.cases
# [1] B A B A A E D B A A B C B A E E A B B B
```

```{r gbmTrainTest, cache=TRUE, eval=FALSE}
# MODEL 4) Try a gbm classifier, training parameters determined thru off-line iteration...
suppressMessages(suppressWarnings(library(gbm)))
suppressMessages(suppressWarnings(library(survival)))
suppressMessages(suppressWarnings(library(splines)))
suppressMessages(suppressWarnings(library(parallel)))
suppressMessages(suppressWarnings(library(plyr)))
startTime <- Sys.time()
#cat(format(startTime, "%T"), "building GBM model started...\n")
# use 5 repeats of 5-fold cross-validation
ctrl <- trainControl(method = "repeatedcv", repeats = 5, number = 5)
set.seed(1477)
mod04.gbm <- train(classe ~ ., data=training, method='gbm',
                   verbose=FALSE, trControl = ctrl)
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building GBM model FINISHED!\n")
time.gbm.minutes <- endTime - startTime
units(time.gbm.minutes) <- "mins"
```

```{r gbmErrorsAccuracy, cache=TRUE, eval=FALSE}
# calc in-sample and out-of-sample error
pred.gbm.insample <- predict(mod04.gbm, newdata = training)
pred.gbm.oosample <- predict(mod04.gbm, newdata = testing)
acc.gbm.insample <- sum(pred.gbm.insample == training$classe) / length(training$classe)
acc.gbm.oosample <- sum(pred.gbm.oosample == testing$classe) / length(testing$classe)
# predict test cases
pred.gbm.test.cases <- predict(mod04.gbm, newdata = test.cases)
validation.acc.gbm <- sum(pred.gbm.test.cases == correct.validation.responses) /
                      length(correct.validation.responses)
# > pred.gbm.test.cases
# [1] B A B A A E D B A A B C B A E E A B B B
```