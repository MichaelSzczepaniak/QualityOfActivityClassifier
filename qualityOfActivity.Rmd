---
title: "Quality of Activity Classification"
author: "Michael Szczepaniak"
date: "November 2015"
output: html_document
---

## Background

People that are into tracking their personal data with a Jawbone Up, Nike FuelBand, Fitbit or whatever activity tracker they use regularly quantify how much of a particular activity they do.  However, they rarely quantify the quality of how they conduct these activities. In this project, the goal was to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Synposis

```{r readExternalData, cache=TRUE, echo=FALSE}
trainFile <- "https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-training.csv"
testFile <- "https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-testing.csv"
rawActitivity <- read.csv(trainFile)
test.cases <- read.csv(testFile)  # final test cases
correct.validation.responses <- c("B", "A", "B", "A", "A", "E", "D", "B", "A", "A",
                                  "B", "C", "B", "A", "E", "E", "A", "B", "B", "B")
```


```{r trimVariables, cache=TRUE, echo=FALSE}
suppressMessages(suppressWarnings(library(caret)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(knitr)))

## Creates a data frame based on the input df with three columns:
## index, ColumnName and FractionMissing.
## index is the column index in df corresponding to ColumnName
## ColumnName is as the name implies: the name the column in df
## FractionMissing is the fraction of values that are missing or NA.
## The closer this value is to 1, the less data the column contains
getFractionMissing <- function(df = rawActitivity) {
    colCount <- ncol(df)
    returnDf <- data.frame(index=1:ncol(df),
                           columnName=rep("undefined", colCount),
                           FractionMissing=rep(-1, colCount),
                           stringsAsFactors=FALSE)
    for(i in 1:colCount) {
        colVector <- df[,i]
        missingCount <- length(which(colVector == "") * 1)
        missingCount <- missingCount + sum(is.na(colVector) * 1)
        returnDf$columnName[i] <- as.character(names(df)[i])
        returnDf$FractionMissing[i] <- missingCount / length(colVector)
    }
    
    return(returnDf)
}

library(caret); library(dplyr)
varsDf <- getFractionMissing()
# get column/var names with > 80% of values not empty or NA
varsWithData <- filter(varsDf, FractionMissing < 0.20)$columnName
# varsWithLittleData <- filter(varsDf, FractionMissing > 0.90)$columnName
# http://stackoverflow.com/questions/10086494#10086494
trimmedActivity <- subset(rawActitivity, select=varsWithData)
# This eliminated 100 out of the 160 vars. Turns out that these same
# 100 var's aren't populated at all in test set which makes it a
# no-brainer to pitch them.
offset = 6; classDesignatorCol <- ncol(trimmedActivity) # last col is class
# convert new_window from factor to int
trimmedActivity$new_window <- as.integer(trimmedActivity$new_window)
correlationMatrix <- cor(trimmedActivity[, (offset+1):classDesignatorCol-1])
#print(correlationMatrix)
# find attributes that are highly corrected (ideally > 0.75)
highlyCorrelated <- sort(findCorrelation(correlationMatrix, cutoff=0.75))
highlyCorrelated <- highlyCorrelated + offset # get cols relative to trimmed df
trimmedActivity2 <- trimmedActivity[,-highlyCorrelated] # remove highly cor vars
trimmedActivity2 <- trimmedActivity2[,-1] # 1st col is just an index
trimmedActivity3 <- trimmedActivity2[,-4:-2] # remove time stamp info
# remove user_name and new_window after looking at importance plot (see below)
trimmedActivity3 <- trimmedActivity3[,-1] # remove user_name
trimmedActivity3 <- trimmedActivity3[,-1] # remove new_window
```

```{r ldaTrainTest, cache=TRUE, echo=FALSE}
# MODEL 1) Start with fast and simple LDA model using caret training defaults
set.seed(1447)
inTrain <- createDataPartition(trimmedActivity3$classe, p=0.60, list=FALSE)
training <- trimmedActivity3[inTrain,]
testing <- trimmedActivity3[-inTrain,]
startTime <- Sys.time()
#cat(format(startTime, "%T"), "building LDA model started...\n")
mod01.lda <- train(classe ~ ., method='lda', data=training) # about 6 sec's
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building LDA model FINISHED!\n")
time.lda.seconds <- endTime - startTime
# calc in-sample and out-of-sample error
pred.lda.insample <- predict(mod01.lda, newdata = training)
pred.lda.oosample <- predict(mod01.lda, newdata = testing)
acc.lda.insample <- sum(pred.lda.insample == training$classe) / length(training$classe)
acc.lda.oosample <- sum(pred.lda.oosample == testing$classe) / length(testing$classe)
# predict test cases
pred.lda.test.cases <- predict(mod01.lda, newdata = test.cases)
validation.acc.lda <- sum(pred.lda.test.cases == correct.validation.responses) /
                      length(correct.validation.responses)
# > pred01.lda.test.cases
# [1] B A A A C C D D A A D A B A E B A B A B
```

```{r rpartTrainTest, cache=TRUE, echo=FALSE}
suppressMessages(suppressWarnings(library(rpart)))
# MODEL 2) Try a classification tree using caret training defaults
#cat(format(Sys.time(), "%T"), "building CART model started...\n")
set.seed(1449)
mod02.cart <- train(classe ~ ., method='rpart', data=training) # about 9 sec's
#cat(format(Sys.time(), "%T"), "building CART model FINISHED!\n")
# calc in-sample and out-of-sample error
pred.cart.insample <- predict(mod02.cart, newdata = training)
pred.cart.oosample <- predict(mod02.cart, newdata = testing)
acc.cart.insample <- sum(pred.cart.insample == training$classe) / length(training$classe)
acc.cart.oosample <- sum(pred.cart.oosample == testing$classe) / length(testing$classe)

# predict test cases
pred.cart.test.cases <- predict(mod02.cart, newdata = test.cases)
validation.acc.cart <- sum(pred.cart.test.cases == correct.validation.responses) /
                       length(correct.validation.responses)
# > pred.cart.test.cases
#  [1] E C A A A C C C A A B C B A C B E B E B
```

```{r rfTrainTest, cache=TRUE, echo=FALSE}
suppressMessages(suppressWarnings(library(randomForest)))
# MODEL 3) Try a random forrest classifier
# cat(format(Sys.time(), "%T"), "building RF model started...\n")
set.seed(1451)
mod03.rf <- train(classe ~ ., method='rf', prox=TRUE, data=training)
# above line took ~190 min's on my Xeon 16Gb workstation to complete
# and gave an in-sample error of < 0.3%
# cat(format(Sys.time(), "%T"), "building RF model FINISHED!\n")
# calc in-sample and out-of-sample error
pred.rf.insample <- predict(mod03.rf, newdata = training)
pred.rf.oosample <- predict(mod03.rf, newdata = testing)
acc.rf.insample <- sum(pred.rf.insample == training$classe) / length(training$classe)
acc.rf.oosample <- sum(pred.rf.oosample == testing$classe) / length(testing$classe)

# read the test cases and predict them
pred.rf.test.cases <- predict(mod03.rf, newdata = test.cases)
# > pred.rf.test.cases
# [1] B A B A A E D B A A B C B A E E A B B B
```

```{r gbmTrainTest, cache=TRUE, echo=FALSE}
# MODEL 4) Try a gbm classifier, training parameters determined thru off-line iteration...
suppressMessages(suppressWarnings(library(gbm)))
suppressMessages(suppressWarnings(library(survival)))
suppressMessages(suppressWarnings(library(splines)))
suppressMessages(suppressWarnings(library(parallel)))
suppressMessages(suppressWarnings(library(plyr)))
#cat(format(Sys.time(), "%T"), "building GBM model started...\n")
# use 5 repeats of 5-fold cross-validation
ctrl <- trainControl(method = "repeatedcv", repeats = 5, number = 5)
set.seed(1477)
mod04.gbm <- train(classe ~ ., data=training,
                   method='gbm',
                   verbose=FALSE,
                   trControl = ctrl) # ~x min's
#cat(format(Sys.time(), "%T"), "building GBM model FINISHED!\n")
# calc in-sample error
pred.gbm.insample <- predict(mod04.gbm, newdata = training)
pred.gbm.oosample <- predict(mod04.gbm, newdata = testing)
acc.gbm.insample <- sum(pred.gbm.insample == training$classe) / length(training$classe)
acc.gbm.oosample <- sum(pred.gbm.oosample == testing$classe) / length(testing$classe)
# predict test cases
pred.gbm.test.cases <- predict(mod04.gbm, newdata = test.cases)
validation.acc.gbm <- sum(pred.gbm.test.cases == correct.validation.responses) /
                      length(correct.validation.responses)
# > pred.gbm.test.cases
# [1] B A B A A E D B A A B C B A E E A B B B
```

A Random Forrest (RF) model with 32 predictors was selected for use to predict 20 final validation test cases.  While the RF model accurately predicted all 20 test cases, a GBM also was able to predict each of the validation cases correctly.  Two other model types were evaluated as part of the investigation: Linear Discriminant Analysis (LDA) and Recursive Partitioning and Regression Tree (RPART).  The LDA and RPART models were eliminated due to high in-sample error rates (`r 100*(1-round(acc.lda.insample, 3))`% and `r 100*(1-round(acc.cart.insample, 3))`% respectively).  The RF model had a better in-sample error rate compared to GBM (`r 100*(1-round(acc.rf.insample, 5))`% vs. `r 100*(1-round(acc.gbm.insample, 5))`% for GBM), but the RF model took over 177 minutes to train using default caret setting while the GBM only took 1 minutes using 5 repeats of 5-fold cross-validation.  The out-of-sample error rates estimated from the testing hold-outs for the GBM and RF models were `r 100*(1-round(acc.gbm.oosample, 3))`% and `r 100*(1-round(acc.rf.oosample, 3))`% respectively.  The GDM was deemed good enough to use on the validation test set.

The code behind all the calculations can be found in [this RMarkdown file](https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/qualityOfActivity.Rmd).

## Variable Selection

The training data was downloaded from [here](https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-training.csv) and the testing data [here](https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-testing.csv).  After downloading the data files, the training data was read and the columns with missing data were removed.  Eliminating the missing and sparsely populated column took the variable count from 160 down to 60.  The next reduction came from identifying and removing highly correlated variables which took the count from 60 down to 39.  The last reduction was the result of removing variables that had either low significance as determined by the *varImp* in caret or just didn't belong in the classifier (user_name, new_window and pitch_arm variables).  

Details regarding the variable reduction process are described in [Appendix A](http://michaelszczepaniak.github.io/QualityOfActivityClassifier/Appendices.html) and are summarized in **Figure 1** below.

```{r variableReductionPath, fig.width=8, fig.height=4, cache=TRUE, echo=FALSE}
suppressMessages(suppressWarnings(library(ggplot2)))
library(ggplot2)
iterNum <- c(0:3)
variableCount <- c(160, 60, 39, 33)  # var counts after each itr
df.var <- data.frame(itr=iterNum, varCount=variableCount)
pFig2 <- ggplot(df.var, aes(x=itr, y=varCount)) + geom_line()
pFig2 <- pFig2 + ggtitle("Figure 1 - Variable Count vs. Iteration Number")
pFig2 <- pFig2 +labs(x="Iteration #", y="Model Variable count")
pFig2 <- pFig2 + coord_cartesian(xlim=0:3) + scale_x_continuous(breaks=0:3)
pFig2 <- pFig2 + annotate("text", x = 0.55, y = 115, color = "darkgreen", angle = -43,
                          label = "Remove empty & sparsely populated variables", size=3.5)
pFig2 <- pFig2 + annotate("text", x = 1.5, y = 55, color = "darkorange1", angle = -11,
                          label = "Remove highly correlated variables", size=3.5)
pFig2 <- pFig2 + annotate("text", x = 2.5, y = 53, color = "blue", angle = 0,
                          label = "Remove low significance variables and", size=3.5)
pFig2 <- pFig2 + annotate("text", x = 2.5, y = 48, color = "blue", angle = 0,
                          label = "variables that shouldn't be predictors", size=3.5)
pFig2
```

The 32 variables selected as preditors for each model evaluated are shown below.

```{r}
names(trimmedActivity3)[-ncol(trimmedActivity3)]
```


## Data Partitioning and Model Builds

After the predictors were selected, the data was partitioned such that 60% of the data was allocated to the training set and 40% to the test set using the code shown below.

```{r eval=FALSE}
set.seed(1447)
inTrain <- createDataPartition(trimmedActivity3$classe, p=0.60, list=FALSE)
training <- trimmedActivity3[inTrain,]
testing <- trimmedActivity3[-inTrain,]
```

Each model was built by passing the **training** set to the *train* function in *caret* package as shown below.

```{r eval=FALSE}
mod01.lda <- train(classe ~ ., method='lda', data=training)    # LDA model build
mod02.cart <- train(classe ~ ., method='rpart', data=training) # RPART model build
# Random Forrest model build
set.seed(1451); mod03.rf <- train(classe ~ ., method='rf', prox=TRUE, data=training)
# GBM model build
ctrl <- trainControl(method = "repeatedcv", repeats = 5, number = 5)
set.seed(1477)
mod04.gbm <- train(classe ~ ., data=training, method='gbm', verbose=FALSE, trControl = ctrl)
```

Training defaults provided by the *caret* package were used for all models initially, but it was found that GBM training speed could be improved significantly (from about 30 minutes to about 12 minutes) without losing much accuracy by using the *trainControl* setting shown above.

## Summary of Results

The table in **Figure 2** below summarizes the results of the model selection and evaluation.

```{r cache=TRUE, echo=FALSE}
# Figure 2 table summarizing results
# Calculating rf final accuracy outside of rf block to avoid having to recompile
# the rf section which takes about 3 hrs.
validation.acc.rf <- sum(pred.rf.test.cases == correct.validation.responses) /
                     length(correct.validation.responses)
rowNameValues <- c("Build Time (min)", "Model In-Memeory Size (Mb)",
                   "In-Sample Error", "Out-of-Sample Error",
                   "Final Test Accuracy")
# build times will vary by machine, model size was read from environment
activity.sum.table <- data.frame(Model=c("LDA", "RPART", "RF", "GBM"),
                                 BuildTime=c(0.1, 0.15, 177.4, 11.8),
                                 ModelSize=c(4, 8, 1001, 12),
                                 InSampleError=c(round(1-acc.lda.insample, 3),
                                                 round(1-acc.cart.insample, 3),
                                                 round(1-acc.rf.insample, 3),
                                                 round(1-acc.gbm.insample, 3)),
                                 OoSampleError=c(round(1-acc.lda.oosample, 3),
                                                 round(1-acc.cart.oosample, 3),
                                                 round(1-acc.rf.oosample, 3),
                                                 round(1-acc.gbm.oosample, 3)),
                                 FinalTestAccuracy=c(round(validation.acc.lda, 4),
                                                     round(validation.acc.cart, 4),
                                                     round(validation.acc.rf, 4),
                                                     round(validation.acc.gbm, 4)))

createResultsTable <- function(qualActData = activity.sum.table) {
    suppressMessages(suppressWarnings(library(knitr)))
    
    df <- data.frame(Model=rowNameValues,
                     LDA=c(as.character(qualActData$BuildTime[1]),
                           as.character(qualActData$ModelSize[1]),
                           as.character(qualActData$InSampleError[1]),
                           as.character(qualActData$OoSampleError[1]),
                           as.character(qualActData$FinalTestAccuracy[1])),
                     RPART=c(as.character(qualActData$BuildTime[2]),
                             as.character(qualActData$ModelSize[2]),
                             as.character(qualActData$InSampleError[2]),
                             as.character(qualActData$OoSampleError[2]),
                             as.character(qualActData$FinalTestAccuracy[2])),
                     RF=c(as.character(qualActData$BuildTime[3]),
                          as.character(qualActData$ModelSize[3]),
                          as.character(qualActData$InSampleError[3]),
                          as.character(qualActData$OoSampleError[3]),
                          as.character(qualActData$FinalTestAccuracy[3])),
                     GBM=c(as.character(qualActData$BuildTime[4]),
                           as.character(qualActData$ModelSize[4]),
                           as.character(qualActData$InSampleError[4]),
                           as.character(qualActData$OoSampleError[4]),
                           as.character(qualActData$FinalTestAccuracy[4])))
                     
    library(knitr) # has kable function to build RMarkdown tables
    options(scipen = 5, digits = 4)
    kable(df, digits=3)
}

createResultsTable()
```

It's worth noting that while the RF model was the most accurate, it took an order of magnitude more time to build and required two orders of magnitude more memory.

## Error Rates

The **In-Sample Error** (IS) rates shown in **Figure 2** were calculated using the predictions from the models built in the **Data Partitioning and Model Builds** section on the **training** data.  The **Out-of-Sample Error** (OOS) rates in the same figure were calculated using the samples held out in the **testing** data set.  The IS error rates were larger than the OOS error rates as one should expect.

As shown in **Figure 2** above, the OOS error rates for the RF and GBM models were `r round(1-acc.rf.oosample, 3)` and `r round(1-acc.gbm.oosample, 3)` respectively.  The code used for calculating the IS errors, OOS errors, and final (validation) test accuracy is listed in [Appendix B](http://michaelszczepaniak.github.io/QualityOfActivityClassifier/Appendices.html#appendix-b---calculation-of-error-and-accuracy)