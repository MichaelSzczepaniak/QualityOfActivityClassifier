---
title: "Quality of Activity Classification"
author: "Michael Szczepaniak"
date: "November 2015"
output: html_document
---

## Background

People that are into tracking their personal data with a Jawbone Up, Nike FuelBand, Fitbit or whatever activity tracker they use regularly quantify how much of a particular activity they do.  However, they rarely quantify the quality of how they conduct these activities. In this project, the goal was to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Synposis

```{r readExternalData, cache=TRUE, echo=FALSE}
trainFile <- "https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-training.csv"
testFile <- "https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-testing.csv"
rawActitivity <- read.csv(trainFile)
test.cases <- read.csv(testFile)  # final test cases
correct.validation.responses <- c("B", "A", "B", "A", "A", "E", "D", "B", "A", "A",
                                  "B", "C", "B", "A", "E", "E", "A", "B", "B", "B")
```


```{r trimVariables, cache=TRUE, echo=FALSE}
suppressMessages(suppressWarnings(library(caret)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(knitr)))

## Creates a data frame based on the input df with three columns:
## index, ColumnName and FractionMissing.
## index is the column index in df corresponding to ColumnName
## ColumnName is as the name implies: the name the column in df
## FractionMissing is the fraction of values that are missing or NA.
## The closer this value is to 1, the less data the column contains
getFractionMissing <- function(df = rawActitivity) {
    colCount <- ncol(df)
    returnDf <- data.frame(index=1:ncol(df),
                           columnName=rep("undefined", colCount),
                           FractionMissing=rep(-1, colCount),
                           stringsAsFactors=FALSE)
    for(i in 1:colCount) {
        colVector <- df[,i]
        missingCount <- length(which(colVector == "") * 1)
        missingCount <- missingCount + sum(is.na(colVector) * 1)
        returnDf$columnName[i] <- as.character(names(df)[i])
        returnDf$FractionMissing[i] <- missingCount / length(colVector)
    }
    
    return(returnDf)
}

library(caret); library(dplyr)
varsDf <- getFractionMissing()
# get column/var names with > 80% of values not empty or NA
varsWithData <- filter(varsDf, FractionMissing < 0.20)$columnName
# varsWithLittleData <- filter(varsDf, FractionMissing > 0.90)$columnName
# http://stackoverflow.com/questions/10086494#10086494
trimmedActivity <- subset(rawActitivity, select=varsWithData)
# This eliminated 100 out of the 160 vars. Turns out that these same
# 100 var's aren't populated at all in test set which makes it a
# no-brainer to pitch them.
offset = 6; classDesignatorCol <- ncol(trimmedActivity) # last col is class
# convert new_window from factor to int
trimmedActivity$new_window <- as.integer(trimmedActivity$new_window)
correlationMatrix <- cor(trimmedActivity[, (offset+1):classDesignatorCol-1])
#print(correlationMatrix)
# find attributes that are highly corrected (ideally > 0.75)
highlyCorrelated <- sort(findCorrelation(correlationMatrix, cutoff=0.75))
highlyCorrelated <- highlyCorrelated + offset # get cols relative to trimmed df
trimmedActivity2 <- trimmedActivity[,-highlyCorrelated] # remove highly cor vars
trimmedActivity2 <- trimmedActivity2[,-1] # 1st col is just an index
trimmedActivity3 <- trimmedActivity2[,-4:-2] # remove time stamp info
# remove user_name and new_window after looking at importance plot (see below)
trimmedActivity3 <- trimmedActivity3[,-1] # remove user_name
trimmedActivity3 <- trimmedActivity3[,-1] # remove new_window
```

```{r ldaTrainTest, cache=TRUE, echo=FALSE}
# MODEL 1) Start with fast and simple LDA model using caret training defaults
set.seed(1447)
inTrain <- createDataPartition(trimmedActivity3$classe, p=0.60, list=FALSE)
training <- trimmedActivity3[inTrain,]
testing <- trimmedActivity3[-inTrain,]
startTime <- Sys.time()
#cat(format(startTime, "%T"), "building LDA model started...\n")
set.seed(374659)
mod01.lda <- train(classe ~ ., method='lda', data=training) # about 6 sec's
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building LDA model FINISHED!\n")
time.lda.minutes <- endTime - startTime
# http://stackoverflow.com/questions/5396429/getting-consist-units-from-diff-command-in-r
units(time.lda.minutes) <- "mins"
```

```{r ldaErrorsAccuracy, cache=TRUE, echo=FALSE}
# calc in-sample and out-of-sample error
pred.lda.insample <- predict(mod01.lda, newdata = training)
pred.lda.oosample <- predict(mod01.lda, newdata = testing)
acc.lda.insample <- sum(pred.lda.insample == training$classe) / length(training$classe)
acc.lda.oosample <- sum(pred.lda.oosample == testing$classe) / length(testing$classe)
# predict test cases
pred.lda.test.cases <- predict(mod01.lda, newdata = test.cases)
validation.acc.lda <- sum(pred.lda.test.cases == correct.validation.responses) /
                      length(correct.validation.responses)
# > pred01.lda.test.cases
# [1] B A A A C C D D A A D A B A E B A B A B
```

```{r rpartTrainTest, cache=TRUE, echo=FALSE}
suppressMessages(suppressWarnings(library(rpart)))
# MODEL 2) Try a classification tree using caret training defaults
startTime <- Sys.time()
#cat(format(Sys.time(), "%T"), "building CART model started...\n")
set.seed(1449)
mod02.cart <- train(classe ~ ., method='rpart', data=training) # about 9 sec's
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building CART model FINISHED!\n")
time.rpart.minutes <- endTime - startTime
units(time.rpart.minutes) <- "mins"
```

```{r rpartErrorsAccuracy, cache=TRUE, echo=FALSE}
# calc in-sample and out-of-sample error
pred.cart.insample <- predict(mod02.cart, newdata = training)
pred.cart.oosample <- predict(mod02.cart, newdata = testing)
acc.cart.insample <- sum(pred.cart.insample == training$classe) / length(training$classe)
acc.cart.oosample <- sum(pred.cart.oosample == testing$classe) / length(testing$classe)

# predict test cases
pred.cart.test.cases <- predict(mod02.cart, newdata = test.cases)
validation.acc.cart <- sum(pred.cart.test.cases == correct.validation.responses) /
                       length(correct.validation.responses)
# > pred.cart.test.cases
#  [1] E C A A A C C C A A B C B A C B E B E B
```

```{r rfTrainTest, cache=TRUE, echo=FALSE}
suppressMessages(suppressWarnings(library(randomForest)))
# MODEL 3) Try a random forrest classifier
startTime <- Sys.time()
# cat(format(startTime, "%T"), "building RF model started...\n")
set.seed(1451)
mod03.rf <- train(classe ~ ., method='rf', prox=TRUE, data=training)
# above line took 3 hrs. on my Xeon 16Gb workstation to complete
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building RF model FINISHED!\n")
time.rf.minutes <- endTime - startTime
units(time.rf.minutes) <- "mins"
```

```{r rfErrorsAccuracy, cache=TRUE, echo=FALSE}
# calc in-sample and out-of-sample error
pred.rf.insample <- predict(mod03.rf, newdata = training)
pred.rf.oosample <- predict(mod03.rf, newdata = testing)
acc.rf.insample <- sum(pred.rf.insample == training$classe) / length(training$classe)
acc.rf.oosample <- sum(pred.rf.oosample == testing$classe) / length(testing$classe)

# read the test cases and predict them
pred.rf.test.cases <- predict(mod03.rf, newdata = test.cases)
validation.acc.rf <- sum(pred.rf.test.cases == correct.validation.responses) /
                     length(correct.validation.responses)
# > pred.rf.test.cases
# [1] B A B A A E D B A A B C B A E E A B B B
```

```{r gbmTrainTest, cache=TRUE, echo=FALSE}
# MODEL 4) Try a gbm classifier, training parameters determined thru off-line iteration...
suppressMessages(suppressWarnings(library(gbm)))
suppressMessages(suppressWarnings(library(survival)))
suppressMessages(suppressWarnings(library(splines)))
suppressMessages(suppressWarnings(library(parallel)))
suppressMessages(suppressWarnings(library(plyr)))
startTime <- Sys.time()
#cat(format(startTime, "%T"), "building GBM model started...\n")
# use 5 repeats of 5-fold cross-validation
ctrl <- trainControl(method = "repeatedcv", repeats = 5, number = 5)
set.seed(1477)
mod04.gbm <- train(classe ~ ., data=training, method='gbm',
                   verbose=FALSE, trControl = ctrl)
endTime <- Sys.time()
#cat(format(endTime, "%T"), "building GBM model FINISHED!\n")
time.gbm.minutes <- endTime - startTime
units(time.gbm.minutes) <- "mins"
```

```{r gbmErrorsAccuracy, cache=TRUE, echo=FALSE}
# calc in-sample error
pred.gbm.insample <- predict(mod04.gbm, newdata = training)
pred.gbm.oosample <- predict(mod04.gbm, newdata = testing)
acc.gbm.insample <- sum(pred.gbm.insample == training$classe) / length(training$classe)
acc.gbm.oosample <- sum(pred.gbm.oosample == testing$classe) / length(testing$classe)
# predict test cases
pred.gbm.test.cases <- predict(mod04.gbm, newdata = test.cases)
validation.acc.gbm <- sum(pred.gbm.test.cases == correct.validation.responses) /
                      length(correct.validation.responses)
# > pred.gbm.test.cases
# [1] B A B A A E D B A A B C B A E E A B B B
```

```{r variableCounts, cache=TRUE, echo=FALSE}
orig.pred.count <- ncol(rawActitivity) - 1
nodata.pred.count <- ncol(trimmedActivity) - 1
nohighcorr.pred.count <- ncol(trimmedActivity2) - 1
final.pred.count <- ncol(trimmedActivity3) - 1
```


A Random Forrest (RF) model with 32 predictors was selected for use to predict 20 final validation test cases.  While the RF model accurately predicted all 20 test cases, a Boosted Tree model (GBM) also was able to predict each of the validation cases correctly.  Two other model types were evaluated as part of the investigation: Linear Discriminant Analysis (LDA) and Recursive Partitioning and Regression Tree (RPART).  The LDA and RPART models were eliminated due to high in-sample error rates (`r 100*(1-round(acc.lda.insample, 3))`% and `r 100*(1-round(acc.cart.insample, 3))`% respectively).  The RF model had a better in-sample error rate compared to GBM (`r 100*(1-round(acc.rf.insample, 5))`% vs. `r 100*(1-round(acc.gbm.insample, 5))`% for GBM), but the RF model took over `r round(time.rf.minutes, 0)` minutes to train using default caret setting while the GBM only took about `r round(time.gbm.minutes, 0)` minutes using 5 repeats of 5-fold cross-validation.  The out-of-sample error rates estimated from the testing hold-outs for the GBM and RF models were `r 100*(1-round(acc.gbm.oosample, 3))`% and `r 100*(1-round(acc.rf.oosample, 3))`% respectively.  The GBM would have been good enough to use on the validation test set.

The code behind all the calculations can be found in [this RMarkdown file](https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/qualityOfActivity.Rmd).

## Variable Selection

The training data was downloaded from [here](https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-training.csv) and the testing data [here](https://github.com/MichaelSzczepaniak/QualityOfActivityClassifier/raw/master/pml-testing.csv).  After downloading the data files, the training data was read and the columns with missing data were removed.  Eliminating the missing and sparsely populated column took the variable count from `r orig.pred.count` down to `r nodata.pred.count`.  The next reduction came from identifying and removing highly correlated variables which took the count from `r nodata.pred.count` down to `r nohighcorr.pred.count`.  

Details regarding the variable reduction process are described in [Appendix A](http://michaelszczepaniak.github.io/QualityOfActivityClassifier/Appendices.html) and are summarized in **Figure 1** below.

```{r variableReductionPath, fig.width=8, fig.height=4, cache=TRUE, echo=FALSE}
suppressMessages(suppressWarnings(library(ggplot2)))
library(ggplot2)
iterNum <- c(0:3)
variableCounts <- c(orig.pred.count, nodata.pred.count, nohighcorr.pred.count, final.pred.count)
df.var <- data.frame(itr=iterNum, varCount=variableCounts)
pFig2 <- ggplot(df.var, aes(x=itr, y=varCount)) + geom_line()
pFig2 <- pFig2 + ggtitle("Figure 1 - Variable Count vs. Iteration Number")
pFig2 <- pFig2 +labs(x="Iteration #", y="Model Variable count")
pFig2 <- pFig2 + coord_cartesian(xlim=0:3) + scale_x_continuous(breaks=0:3)
pFig2 <- pFig2 + annotate("text", x = 0.55, y = 115, color = "darkgreen", angle = -43,
                          label = "Remove empty & sparsely populated variables", size=3.5)
pFig2 <- pFig2 + annotate("text", x = 1.5, y = 55, color = "darkorange1", angle = -11,
                          label = "Remove highly correlated variables", size=3.5)
pFig2 <- pFig2 + annotate("text", x = 2.5, y = 53, color = "blue", angle = 0,
                          label = "Remove low significance variables and", size=3.5)
pFig2 <- pFig2 + annotate("text", x = 2.5, y = 48, color = "blue", angle = 0,
                          label = "variables that shouldn't be predictors", size=3.5)
pFig2
```

The last reduction was the result of removing variables that had low significance as determined by the *varImp* function in caret.  The variables **user_name**, **new_window** and **pitch_arm variables** were eliminated because they had very lower importance than **gyro_arm_z** as shown in **Figure 2** below.  All of the `r final.pred.count` variables selected as preditors for the final are shown on the y-axis.

```{r predictorImportance, cache=TRUE}
importance <- varImp(mod03.rf, scale=FALSE)
#print(importance)
plot(importance, main="Figure 2 - Variable Importance in the Final RF Model")
```

## Data Partitioning and Model Builds

After the predictors were selected, the data was partitioned such that 60% of the data was allocated to the training set and 40% to the test set using the code shown below.

```{r eval=FALSE}
set.seed(1447)
inTrain <- createDataPartition(trimmedActivity3$classe, p=0.60, list=FALSE)
training <- trimmedActivity3[inTrain,]
testing <- trimmedActivity3[-inTrain,]
```

Each model was built by passing the **training** set to the *train* function in *caret* package as shown below.  For details see [Appendix B](http://michaelszczepaniak.github.io/QualityOfActivityClassifier/Appendices.html#appendix-b---calculation-of-error-and-accuracy).

```{r eval=FALSE}
set.seed(374659)
mod01.lda <- train(classe ~ ., method='lda', data=training)    # LDA model build
# ...
set.seed(1449)
mod02.cart <- train(classe ~ ., method='rpart', data=training) # RPART model build
# ...
# Random Forrest model build
set.seed(1451); mod03.rf <- train(classe ~ ., method='rf', prox=TRUE, data=training)
# ...
# GBM (boosted tree) model build
ctrl <- trainControl(method = "repeatedcv", repeats = 5, number = 5)
set.seed(1477)
mod04.gbm <- train(classe ~ ., data=training, method='gbm', verbose=FALSE, trControl = ctrl)
```

Training defaults provided by the *caret* package were used for all models initially, but it was found that GBM training speed could be improved significantly (from about 30 minutes to about 12 minutes) without losing much accuracy by using the *trainControl* setting shown above.

## Summary of Results

The table in **Figure 3** below summarizes the results of the model selection and evaluation.

```{r cache=TRUE, echo=FALSE}
# Figure 3 table summarizing results
rowNameValues <- c("Build Time (min)", "Model In-Memeory Size (Mb)",
                   "In-Sample Error", "Out-of-Sample Error",
                   "Final Test Accuracy")
# build times will vary by machine, model size was read from environment
buildTimes <- round(c(time.lda.minutes, time.rpart.minutes,
                      time.rf.minutes, time.gbm.minutes), 2)
modelSizes <- round(c(object.size(mod01.lda)/2^20, object.size(mod02.cart)/2^20,
                      object.size(mod03.rf)/2^20, object.size(mod04.gbm)/2^20), 2)
isErrors <- round(c((1-acc.lda.insample), (1-acc.cart.insample),
                    (1-acc.rf.insample), (1-acc.gbm.insample)), 4)
oosErrors <- round(c((1-acc.lda.oosample), (1-acc.cart.oosample),
                     (1-acc.rf.oosample), (1-acc.gbm.oosample)), 4)
validationTestAcc <- round(c(validation.acc.lda, validation.acc.cart,
                             validation.acc.rf, validation.acc.gbm), 4)
# build data frame for the table
activity.sum.table <- data.frame(Model=c("LDA", "RPART", "RF", "GBM"),
                                 BuildTime=buildTimes,
                                 ModelSize=modelSizes,
                                 InSampleError=isErrors,
                                 OoSampleError=oosErrors,
                                 FinalTestAccuracy=validationTestAcc)

createResultsTable <- function(qualActData = activity.sum.table) {
    suppressMessages(suppressWarnings(library(knitr)))
    
    df <- data.frame(Model=rowNameValues,
                     LDA=c(as.character(qualActData$BuildTime[1]),
                           as.character(qualActData$ModelSize[1]),
                           as.character(qualActData$InSampleError[1]),
                           as.character(qualActData$OoSampleError[1]),
                           as.character(qualActData$FinalTestAccuracy[1])),
                     RPART=c(as.character(qualActData$BuildTime[2]),
                             as.character(qualActData$ModelSize[2]),
                             as.character(qualActData$InSampleError[2]),
                             as.character(qualActData$OoSampleError[2]),
                             as.character(qualActData$FinalTestAccuracy[2])),
                     RF=c(as.character(qualActData$BuildTime[3]),
                          as.character(qualActData$ModelSize[3]),
                          as.character(qualActData$InSampleError[3]),
                          as.character(qualActData$OoSampleError[3]),
                          as.character(qualActData$FinalTestAccuracy[3])),
                     GBM=c(as.character(qualActData$BuildTime[4]),
                           as.character(qualActData$ModelSize[4]),
                           as.character(qualActData$InSampleError[4]),
                           as.character(qualActData$OoSampleError[4]),
                           as.character(qualActData$FinalTestAccuracy[4])))
                     
    library(knitr) # has kable function to build RMarkdown tables
    options(scipen = 5, digits = 4)
    kable(df, digits=3)
}

createResultsTable()
```

It's worth noting that while the RF model was the most accurate, it took an order of magnitude more time to build and required two orders of magnitude more memory.

## Error Rates

The **In-Sample Error** (IS) rates shown in **Figure 3** were calculated using the predictions from the models built in the **Data Partitioning and Model Builds** section on the **training** data.  The **Out-of-Sample Error** (OOS) rates in the same figure were calculated using the samples held out in the **testing** data set.  The IS error rates were very close to the OOS error rates.  The IS error rates were larger than OOS error rates in 3 of the 4 cases as one would expect.  The RPART anomaly appears to be due to resampling error.

As shown in **Figure 3** above, the OOS error rates for the RF and GBM models were `r round(1-acc.rf.oosample, 3)` and `r round(1-acc.gbm.oosample, 3)` respectively.  The code used for calculating the IS errors, OOS errors, and final (validation) test accuracy is listed in [Appendix B](http://michaelszczepaniak.github.io/QualityOfActivityClassifier/Appendices.html#appendix-b---calculation-of-error-and-accuracy)

## Final Model

Note that the OOB estimate of error rate is little lower than the OOS estimate reported in **Figure 3** above.  [This post quotes](http://stackoverflow.com/questions/18541923/what-is-out-of-bag-error-in-random-forests#24663120) a paper by Breiman which reports emperical evidence that OOB (an in-sample error estimate) is as good as holding out a test set (OOS error estimate).  The results I saw are certainly very close.

```{r finalModel, cache=TRUE}
mod03.rf$finalModel
```

